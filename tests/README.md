# Test Suite for MSE Telegram Bot

## üöÄ Overview

This test suite provides comprehensive coverage for the MSE Telegram Bot, focusing on the critical components introduced in PR #18 (AI insights system).

## üìã Test Structure

```
tests/
‚îú‚îÄ‚îÄ setup.js                           # Global test setup and mocks
‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îî‚îÄ‚îÄ insights.test.js              # /insights command integration tests
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ ai-insights-service.test.js   # AI insights service unit tests
‚îÇ   ‚îú‚îÄ‚îÄ pattern-analysis-service.test.js # Pattern analysis unit tests
‚îÇ   ‚îî‚îÄ‚îÄ predictive-analytics-service.test.js # Predictive analytics tests
‚îî‚îÄ‚îÄ README.md                         # This file
```

## üß™ Running Tests

### Install dependencies:
```bash
npm install
```

### Run all tests:
```bash
npm test
```

### Run tests in watch mode:
```bash
npm run test:watch
```

### Generate coverage report:
```bash
npm run test:coverage
```

## üìä Test Coverage

### High Priority (PR #18 Critical Path):
- ‚úÖ **AIInsightsService** - Tests AI integration, fallback logic, error handling
- ‚úÖ **PatternAnalysisService** - Tests behavioral pattern detection and analysis
- ‚úÖ **PredictiveAnalyticsService** - Tests forecasting and trend analysis
- ‚úÖ **Insights Command** - Tests end-to-end user interaction flow

### Key Test Scenarios:

#### AI Integration Tests:
- ‚úÖ OpenAI API success and failure scenarios
- ‚úÖ Anthropic fallback when OpenAI fails
- ‚úÖ Graceful degradation when both AI services fail
- ‚úÖ Malformed AI response handling
- ‚úÖ Rate limiting and error recovery

#### Pattern Analysis Tests:
- ‚úÖ Time pattern detection (best/worst hours, day of week)
- ‚úÖ Activity pattern analysis (mood boosters, energy drains)
- ‚úÖ Social context preferences
- ‚úÖ Correlation analysis between metrics
- ‚úÖ Anomaly detection
- ‚úÖ Empty data handling

#### Predictive Analytics Tests:
- ‚úÖ Time-based predictions
- ‚úÖ Trend analysis (improving, declining, stable)
- ‚úÖ Activity recommendations
- ‚úÖ Flow state predictions
- ‚úÖ Confidence scoring
- ‚úÖ Insufficient data scenarios

#### Command Integration Tests:
- ‚úÖ Full user flow with sufficient data
- ‚úÖ New user experience (insufficient data)
- ‚úÖ Unregistered user handling
- ‚úÖ Service failure scenarios
- ‚úÖ Interactive button functionality

## üîß Test Configuration

### Environment Variables:
Tests run with mocked environment variables in `tests/setup.js`:
- `NODE_ENV=test`
- `BOT_TOKEN=test_bot_token`
- `MONGODB_URI=mongodb://localhost:27017/mse_bot_test`
- `OPENAI_API_KEY=test_openai_key`
- `ANTHROPIC_API_KEY=test_anthropic_key`

### Global Mocks:
- Console methods (to reduce test noise)
- Telegram Bot API methods
- Database models
- External AI services

### Test Helpers:
- `createMockUser()` - Creates standardized mock user objects
- `createMockResponse()` - Creates mock survey response data
- `createMockBot()` - Creates mock Telegram bot instance

## üéØ Quality Metrics

### Expected Coverage:
- **Services**: >90% line coverage
- **Commands**: >85% line coverage
- **Critical paths**: 100% coverage

### Test Quality Standards:
- Each test focuses on a single responsibility
- Clear test names describing the scenario
- Comprehensive error scenario coverage
- Real-world data patterns in mocks
- Isolated tests with proper setup/teardown

## üö® Critical Test Cases

### Must-Pass Scenarios:
1. **AI Service Failures** - System must gracefully degrade
2. **Insufficient Data** - Appropriate user guidance
3. **Malformed Data** - Robust error handling
4. **Rate Limiting** - Proper retry and fallback logic
5. **User Experience** - Clear messaging and helpful responses

### Performance Considerations:
- Tests use mocked external services (no real API calls)
- Database queries are mocked for speed
- Test timeout set to 30 seconds for complex scenarios

## üìà Continuous Improvement

### Adding New Tests:
1. Follow existing naming conventions
2. Use provided test helpers
3. Mock external dependencies
4. Include both success and failure scenarios
5. Update this README with new test descriptions

### Test Maintenance:
- Run tests before each commit
- Update tests when modifying services
- Keep mocks realistic and up-to-date
- Monitor coverage reports

## üîç Debugging Tests

### Common Issues:
- **Async/Await**: Ensure all async operations are properly awaited
- **Mock Cleanup**: Use `jest.clearAllMocks()` in `beforeEach`
- **Timeout Issues**: Check for unresolved promises
- **Mock Conflicts**: Verify mocks don't interfere with each other

### Debug Commands:
```bash
# Run specific test file
npm test insights.test.js

# Run with verbose output
npm test -- --verbose

# Run in debug mode
node --inspect-brk node_modules/.bin/jest --runInBand
```

---

This test suite ensures the reliability and robustness of the AI-powered insights system, providing confidence for production deployment and future development.